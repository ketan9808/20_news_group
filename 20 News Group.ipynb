{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import names\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Training & Testing Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = fetch_20newsgroups(subset = \"all\", shuffle = True, random_state = 10)\n",
    "\n",
    "# bunch.data\n",
    "# bunch.target\n",
    "# bunch.filename\n",
    "# bunch.DESCR\n",
    "# bunch.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data = []\n",
    "for t in list(news_data.target):\n",
    "    if t not in unique_data:\n",
    "        unique_data.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data.sort()\n",
    "print(unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mail in news_data.data[0:5]:\n",
    "    print(mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = fetch_20newsgroups(subset = 'train')\n",
    "X_test = fetch_20newsgroups(subset = 'test')\n",
    "# print(len(X_train.data))        11314\n",
    "# print(len(X_test.data))         7532\n",
    "\n",
    "X_tar = X_train.target\n",
    "Y_tar = X_test.target\n",
    "\n",
    "#print(np.unique(X_tar))        0-19\n",
    "\n",
    "# Important attributes of bunch object\n",
    "# dictionary like object\n",
    "# data - data to learn\n",
    "# target - classification labels\n",
    "# target_names - the meaning of label\n",
    "# fearure_names - the meaning of the feature\n",
    "# DESCR - full description of dataset\n",
    "# filename - physical location of iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.data[0],X_tar.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = set(names.words())\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def clean(data):\n",
    "    cleaned = defaultdict(list)\n",
    "    count = 0\n",
    "    for group in data:\n",
    "        for words in group.split():\n",
    "            if words.isalpha() and words not in all_names:\n",
    "                cleaned[count].append(ps.stem(words.lower()))\n",
    "        cleaned[count] = ' '.join(cleaned[count])\n",
    "        count +=1 \n",
    "    return(list(cleaned.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print(X_train.data[1])\n",
    "\n",
    "x_train_clean = clean(X_train.data)\n",
    "x_test_clean = clean(X_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words = 'english')\n",
    "cv_train = cv.fit_transform(x_train_clean)\n",
    "print(cv.inverse_transform(cv_train[0]))\n",
    "\n",
    "print('-'*127)\n",
    "\n",
    "tf = TfidfVectorizer(stop_words = \"english\", max_features = 10000)      #To make every words a feature\n",
    "x_Train_ = tf.fit_transform(x_train_clean)\n",
    "print(tf.inverse_transform(x_Train_[0]))\n",
    "\n",
    "y_Train_ = tf.transform(x_test_clean)\n",
    "\n",
    "# if (cv.inverse_transform(cv_train[0])==tf.inverse_transform(x_Train_[0])):\n",
    "#    print('Ther are equal')\n",
    "# This is error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "clf = LinearSVC()\n",
    "parameters = {'C' : [1,10,100,1000], 'gamma' : [0.1, 0.01, 0.001, 0.00001]}\n",
    "\n",
    "grid_search_ = GridSearchCV(clf, parameters, n_jobs = -1, cv = 3)\n",
    "\n",
    "grid_search_.fit(x_Train_, X_tar)\n",
    "\n",
    "final = timeit.default_timer()-start_time\n",
    "print(\"Execution Time : \", final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
